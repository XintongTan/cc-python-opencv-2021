{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on a tutorial by Murtaza Hassan, modified and annotaded by Franziska Mack for Parsons Creative Coding: Python Fall 2020.\n",
    "\n",
    "# Image Classifier\n",
    "\n",
    "This classifier is able to detect multiple 2-d query images in a webcam videostream.\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with image paths\n",
    "paths = ['./img/query/magazine.jpg', './img/query/design_anthropology.jpg']\n",
    "# set corresponding labels\n",
    "class_names = ['Garden & Gun', 'Design Anthropology']\n",
    "\n",
    "# create empty list to hold images\n",
    "images = []\n",
    "\n",
    "# reading in all image paths with imread()\n",
    "for path in paths:\n",
    "    img = cv2.imread(path, 0)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a function for finding the keypoints and descriptors of our query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititialize detector\n",
    "orb = cv2.ORB_create(nfeatures=1000)\n",
    "\n",
    "def findDescriptors(images):\n",
    "    # create empty list for descriptors\n",
    "    des_list = []\n",
    "    \n",
    "    # populate descriptor list\n",
    "    # for every image in the image list\n",
    "    for img in images:\n",
    "        # find keypoints and descriptors\n",
    "        kp, des = orb.detectAndCompute(img, None)\n",
    "        # append to descriptor list\n",
    "        des_list.append(des)\n",
    "    return des_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Let's test our function and look at the descriptor list\n",
    "des_list = findDescriptors(images)\n",
    "print(len(des_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the videostream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize videostream\n",
    "vs = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compare the descriptors of each frame to the descriptors of our query images to find possible matches. We'll define a function that finds keypoints and descriptors in our video frame and returns different labels depending on whether any of our query images could be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findID(frame, des_list):\n",
    "    # find keypoints and descriptors in frame\n",
    "    kp2, des2 = orb.detectAndCompute(frame, None)\n",
    "    # initialize matcher\n",
    "    bf = cv2.BFMatcher()\n",
    "    \n",
    "    # we'll use this list to compare the number of good matches found\n",
    "    # for the different query images\n",
    "    match_list = []\n",
    "    final_val = -1\n",
    "    \n",
    "    # exception handling with try and except\n",
    "    try:\n",
    "        # iterate over the descriptors in the descriptor list\n",
    "        for des in des_list:\n",
    "            # use the k-nearest neighbour algorithm to match keypoints\n",
    "            matches = bf.knnMatch(des, des2, k=2)\n",
    "            good_matches = []\n",
    "\n",
    "            # check for ambigous matches\n",
    "            # populate list with those that are not\n",
    "            for m, n in matches:\n",
    "                if m.distance < n.distance * 0.75:\n",
    "                    good_matches.append([m])\n",
    "                    \n",
    "            # store the number of good matches found \n",
    "            # for respective descriptor in match list\n",
    "            match_list.append(len(good_matches))\n",
    "    \n",
    "        # did we find a high number of good matches for any of the descriptors?\n",
    "        # make sure match list is not empty:\n",
    "        if len(match_list) != 0:\n",
    "            # are any of the values higher than 15?\n",
    "            if max(match_list) > 20:\n",
    "                # if so, record the index of the highest value\n",
    "                # we'll use it later to find the matching class label\n",
    "                final_val = match_list.index(max(match_list))\n",
    "    \n",
    "    # if try raises an error, pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return final_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll loop over every frame of our videostream, applying our custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # grab the frame from the videostream\n",
    "    success, frame = vs.read()\n",
    "    # make a copy of it for later (to retain color)\n",
    "    original_frame = frame.copy()\n",
    "    # convert to grayscale image (for feature detection)\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # passing the current frame into our findID function\n",
    "    # if any matches are detected, \n",
    "    # it will yield the corresponding class label\n",
    "    val = findID(frame, des_list)\n",
    "    \n",
    "    # if matches were detected:\n",
    "    if val != -1:\n",
    "        # draw the label on the color frame:\n",
    "        # the correct label is stored in our class_names list at index [val]\n",
    "        cv2.putText(original_frame, class_names[val], (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    " \n",
    "    # show frame with superimposed label\n",
    "    cv2.imshow('output', original_frame)\n",
    "\n",
    "    # if the 'q' key is pressed, break from the loop\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
